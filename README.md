# distributed_train

## 框架

### DOCKER

#### (1)环境隔离
          我们把一系列的基础库、运算库和深度学习库打包成Docker镜像，充分利用Docker的分层机制在不同层面进行共享和组合。Docker隔离了系统环境和执行环境，即隔离同一台服务器上不同训练任务的环境。我们可以把同一个任务分发到带有不同型号GPU卡的服务器上，也可以在同一台服务器上同时运行不同CUDA版本、不同深度学习框架的多个任务。比较完美的解决了上述问题。


#### (2)系统架构和实现

数据中心 ---------DgTrainer/ Mesos + Marathon 集群  --------Docker 仓库

数据中心：使用分布式网络存储
Docker仓库： 存放训练所需要的深度学习框架镜像  比如：Caffe，MxNet。 
中间包括：
1）低层搭建的Mesos+ Marathon 的集群，
2）上层的业务模块DgTrainer，用于提供Web, API, 准备环境和数据，以及调用下层的集群API启停，和监控训练任务（FS,CLUSTER, DOCKER,Task， log）



流程：
1：DgTrainer接受训练参数
2：在数据中心准备数据
3：Docker仓库中准备镜像
4：调用marathon接口，启动训练任务
5：mesos调度任务到合适的slave节点上
6:slave节点从数据中心读取（预读取）数据
7:slave节点从Docker仓库下载对应镜像，资源映射并执行任务
8:记录日志和训练结果，并反馈到DgTrainer


### 容器GPU映射
如果要在Docker 内使用GPU硬件， 需要做GPU的设备映射。 这可以通过--device参数完成。其中nvidia0表示映射第一个GPU卡，nvidia3表示映射的第四个GPU卡


### 分布式训练
目前的深度学习框架大都支持分布式训练。分布式训练分为模型并行和数据并行。大多数采用数据并行方式。把相同的模型分布在不同的计算点上，通过提高batch size 从而加速训练过程。因此，一个训练平台需要支持分布式训练的能力。技术实现上：
（1）依赖具体的深度学习框架Tensorflow， caffe，结合不同的分布式技术，比如MPI、Spark、ParameterServer等
（2）二是需要定义更复杂的资源调度策略，避免出现“木桶效应”
（3）支持更复杂的调试、调优等能力。整体来讲，分布式训练+分布式平台是趋势和必然，也有更多的技术挑战



### Tensorflow的分布式
tensorflow的分布式没有用参数服务器，而是数据流图。 有两种架构模式：
#### (1) in-graph 模式：
多机多GPU, 数据分发仍然在一个节点。配置简单，其他多机多GPU计算节点，只要起个join操作，暴露一个网络接口，等待接受任务即可。这些计算节点暴露出来的网络接口，使用起来和本机的一个GPU的使用一样，只要在操作的时候指定tf.device("/job:worker/task:n) , 就可以向指定GPU一样，把操作指定到一个计算节点上计算，使用起来和多GPU类似，但是这样的坏处是训练数据的分发依然在一个节点上，要把训练数据分发到不同的机器上，严重影响兵法训练速度，在大数据训练的情况下，不推荐使用这种模式
#### (2)between-graph 模式
训练的参数保存在参数服务器，数据不用分发，数据分片的计算把偶存在各个计算节点，各个计算节点自己算自己的，算完之后，把更新的参数告诉参数服务器，其更新参数。模式的优点在于不用训练数据的分发了，尤其是在数据量在TB级别时候，极大节省大量的时间。 推荐使用








