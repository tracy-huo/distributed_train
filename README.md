# distributed_train

## 框架

### DOCKER

#### (1)环境隔离
          我们把一系列的基础库、运算库和深度学习库打包成Docker镜像，充分利用Docker的分层机制在不同层面进行共享和组合。Docker隔离了系统环境和执行环境，即隔离同一台服务器上不同训练任务的环境。我们可以把同一个任务分发到带有不同型号GPU卡的服务器上，也可以在同一台服务器上同时运行不同CUDA版本、不同深度学习框架的多个任务。比较完美的解决了上述问题。


#### (2)系统架构和实现

数据中心 ---------DgTrainer/ Mesos + Marathon 集群  --------Docker 仓库

数据中心：使用分布式网络存储
Docker仓库： 存放训练所需要的深度学习框架镜像  比如：Caffe，MxNet。 
中间包括：
1）低层搭建的Mesos+ Marathon 的集群，
2）上层的业务模块DgTrainer，用于提供Web, API, 准备环境和数据，以及调用下层的集群API启停，和监控训练任务（FS,CLUSTER, DOCKER,Task， log）



流程：
1：DgTrainer接受训练参数
2：在数据中心准备数据
3：Docker仓库中准备镜像
4：调用marathon接口，启动训练任务
5：mesos调度任务到合适的slave节点上
6:slave节点从数据中心读取（预读取）数据
7:slave节点从Docker仓库下载对应镜像，资源映射并执行任务
8:记录日志和训练结果，并反馈到DgTrainer


### 容器GPU映射
如果要在Docker 内使用GPU硬件， 需要做GPU的设备映射。 这可以通过--device参数完成。其中nvidia0表示映射第一个GPU卡，nvidia3表示映射的第四个GPU卡


### 分布式训练
目前的深度学习框架大都支持分布式训练。分布式训练分为模型并行和数据并行。大多数采用数据并行方式。把相同的模型分布在不同的计算点上，通过提高batch size 从而加速训练过程。因此，一个训练平台需要支持分布式训练的能力。技术实现上：
（1）依赖具体的深度学习框架Tensorflow， caffe，结合不同的分布式技术，比如MPI、Spark、ParameterServer等
（2）二是需要定义更复杂的资源调度策略，避免出现“木桶效应”
（3）支持更复杂的调试、调优等能力。整体来讲，分布式训练+分布式平台是趋势和必然，也有更多的技术挑战








